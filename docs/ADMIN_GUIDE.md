# üîç Admin Review Dashboard - User Guide

## Purpose

The Review Dashboard helps identify cases where the model is **uncertain** or **makes mistakes**, which are perfect for:
- üéØ Adjusting detection rules
- üìù Improving AI prompts
- üèãÔ∏è Adding to training dataset

## What Gets Flagged

### 1. Uncertain Cases (Risk 40-60%)
Messages where the model isn't confident:
```
Risk Score: 45% - Model can't decide if it's scam or not
‚Üí Perfect for reviewing detection logic
```

### 2. Incorrect Feedback
Messages where users clicked "‚ùå ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á":
```
3+ incorrect feedback = HIGH PRIORITY
‚Üí Model is clearly wrong, needs immediate attention
```

## Priority Levels

| Priority | Criteria | Action |
|----------|----------|--------|
| üî¥ HIGH | 3+ incorrect feedback | Review immediately, add to training |
| üü° MEDIUM | 1-2 incorrect feedback | Review when possible |
| üü¢ LOW | Just uncertain (40-60%) | Review for edge cases |

## How to Use

### Access the Dashboard

```
URL: http://localhost:8000/admin/review.html
Auth: Enter admin token when prompted
```

### Review Workflow

1. **Check Stats** - See how many cases need review
2. **Filter Cases** - Use filter buttons:
   - "All Cases" - Everything
   - "Uncertain Only" - Risk 40-60%
   - "Incorrect Feedback" - User-reported errors
   - "High Priority" - Most urgent cases

3. **Review Each Case:**
   - Check risk score and category
   - See incorrect feedback count
   - Note the channel (SMS, LINE, etc.)

4. **Take Action:**
   - üè∑Ô∏è "Add to Training" - Mark for model retraining
   - üìù "Mark Reviewed" - Track what you've checked

5. **Export Data:**
   - Click "üìä Export for Training"
   - Gets JSON file with all cases
   - Use for model improvement

## API Endpoint

```bash
# Get uncertain cases
curl http://localhost:8000/v1/admin/review/uncertain?limit=50 \
  -H "Authorization: Bearer admin-secret-key-2024"
```

**Parameters:**
- `limit` - Max results (default: 50)
- `include_feedback` - Include incorrect feedback (default: true)

**Response:**
```json
{
  "total": 15,
  "uncertain_count": 8,
  "incorrect_feedback_count": 7,
  "cases": [
    {
      "request_id": "abc-123",
      "risk_score": 0.52,
      "category": "normal",
      "incorrect_feedback_count": 3,
      "priority": "high",
      "reason": "incorrect_feedback"
    }
  ]
}
```

## Model Improvement Process

### Step 1: Identify Patterns
Look for common themes in uncertain/incorrect cases:
- Specific scam types being missed
- Normal messages flagged as scams
- New scam patterns not in training data

### Step 2: Adjust Rules
For pattern-based fixes:
- Update keywords in `scam_classifier.py`
- Adjust risk thresholds
- Add new detection patterns

### Step 3: Improve Prompts
For AI explanation issues:
- Update prompts in `llm_explainer.py`
- Add examples for edge cases
- Clarify category definitions

### Step 4: Build Training Set
For model retraining:
```bash
# Export cases
curl http://localhost:8000/v1/admin/review/uncertain?limit=1000 \
  -H "Authorization: Bearer admin-token" \
  > training_candidates.json

# Review and label
# Add to training dataset
# Retrain model with new examples
```

## Example Use Cases

### Case 1: Uncertain Message
```
Message: "‡πÇ‡∏≠‡∏ô‡πÄ‡∏á‡∏¥‡∏ô 100 ‡∏ö‡∏≤‡∏ó ‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≤‡∏ß"
Risk: 48% (uncertain)
Category: normal

Action: Check if "‡πÇ‡∏≠‡∏ô‡πÄ‡∏á‡∏¥‡∏ô" triggers false positives
‚Üí Adjust keyword weights or add context rules
```

### Case 2: High Incorrect Feedback
```
Message: [Legitimate bank notification]
Risk: 85% (high)
Category: fake_officer
Incorrect Feedback: 5 users

Action: This is clearly wrong!
‚Üí Add to training set as "normal" example
‚Üí Review bank notification patterns
```

### Case 3: Edge Case
```
Message: "‡∏•‡∏á‡∏ó‡∏∏‡∏ô Bitcoin ‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô"
Risk: 55% (uncertain)
Category: investment_scam

Action: Personal investment vs scam distinction
‚Üí Add context: "‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô" = likely legit
‚Üí Improve keyword context analysis
```

## Dashboard Features

### Visual Indicators
- üî¥ Red border = High priority (3+ incorrect feedback)
- üü° Yellow border = Medium priority (1-2 feedback)
- üü¢ Green border = Low priority (just uncertain)

### Stats Overview
- Total cases needing review
- Count of uncertain predictions
- Count with incorrect feedback

### Sorting
Cases are sorted by:
1. Priority (High ‚Üí Medium ‚Üí Low)
2. Incorrect feedback count (More ‚Üí Less)
3. Uncertainty (Closer to 50% first)

## Tips for Effective Review

1. **Start with High Priority** - These are definite mistakes
2. **Look for Patterns** - Don't review one-by-one, find themes
3. **Document Findings** - Note what needs changing
4. **Batch Updates** - Fix similar issues together
5. **Test Changes** - Verify improvements with test cases

## Data Privacy Note

‚ö†Ô∏è **Important:** Message hashes are shown, not original text
- Cannot see actual message content
- Privacy preserved while still useful for analysis
- Use metadata (risk, category, feedback) for decisions

## Next Steps After Review

1. **Quick Fixes** (Today):
   - Adjust problematic keywords
   - Update thresholds if needed

2. **Medium Changes** (This Week):
   - Update AI prompts
   - Add new detection patterns
   - Refine category definitions

3. **Model Retraining** (This Month):
   - Collect 100+ labeled cases
   - Create training dataset
   - Retrain and A/B test new model

## Monitoring

Track improvement over time:
```bash
# Check if uncertain cases are decreasing
Week 1: 50 uncertain cases
Week 2: 35 uncertain cases (improving!)
Week 3: 20 uncertain cases (much better!)

# Check if incorrect feedback is dropping
Month 1: 15% incorrect feedback rate
Month 2: 8% incorrect feedback rate (success!)
```

## Summary

**The Review Dashboard is your model improvement command center!**

Use it to:
- ‚úÖ Find what the model struggles with
- ‚úÖ Collect cases for retraining
- ‚úÖ Track improvement progress
- ‚úÖ Build a better scam detector

**Start reviewing** ‚Üí **Make improvements** ‚Üí **Deploy** ‚Üí **Repeat!**
